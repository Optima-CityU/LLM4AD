"""
LLM4AD Algorithm Runner - ç®—æ³•è®¾è®¡è¿è¡Œå™¨
å°è£… LLM4AD çš„è¿è¡Œé€»è¾‘ï¼Œæ”¯æŒæµå¼è¾“å‡ºè®¾è®¡è¿‡ç¨‹
"""

import os
import sys
import time
import json
import threading
import queue
from datetime import datetime
from typing import Dict, Any, Generator, Optional
from io import StringIO

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pytz


class OutputCapture:
    """æ•è·æ ‡å‡†è¾“å‡ºç”¨äºæµå¼æ˜¾ç¤º"""
    
    def __init__(self, output_queue: queue.Queue):
        self.output_queue = output_queue
        self._original_stdout = sys.stdout
        self._buffer = StringIO()
    
    def write(self, text):
        self._original_stdout.write(text)
        self._buffer.write(text)
        # è§£æè¾“å‡ºå¹¶æ”¾å…¥é˜Ÿåˆ—
        if text.strip():
            self.output_queue.put({
                "type": "output",
                "text": text
            })
    
    def flush(self):
        self._original_stdout.flush()
        self._buffer.flush()
    
    def get_captured(self):
        return self._buffer.getvalue()


class StreamingProfiler:
    """æµå¼è¾“å‡ºçš„ Profilerï¼Œç”¨äºæ•è·ç®—æ³•è®¾è®¡è¿‡ç¨‹"""
    
    def __init__(self, output_queue: queue.Queue, base_profiler=None):
        self.output_queue = output_queue
        self.base_profiler = base_profiler
        self._num_samples = 0
        self._best_score = float('-inf')
        self._best_function = None
        self._best_program = None
        self._start_time = time.time()
        self._last_update_time = time.time()
    
    def register_function(self, function, program: str = '', **kwargs):
        """æ³¨å†Œä¸€ä¸ªè¯„ä¼°è¿‡çš„å‡½æ•° - è¿™æ˜¯æ ¸å¿ƒçš„æµå¼è¾“å‡ºç‚¹"""
        self._num_samples += 1
        score = function.score
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„æœ€ä½³
        is_new_best = False
        if score is not None and score > self._best_score:
            self._best_score = score
            self._best_function = function
            self._best_program = program
            is_new_best = True
        
        # è®¡ç®—è€—æ—¶
        current_time = time.time()
        elapsed = current_time - self._start_time
        iter_time = current_time - self._last_update_time
        self._last_update_time = current_time
        
        # è·å–ä»£ç å†…å®¹å’Œç®—æ³•æè¿°
        code_str = None
        algorithm_desc = None
        docstring = None
        if function is not None:
            try:
                code_str = str(function)
            except:
                code_str = None
            # è·å–ç®—æ³•æè¿°
            try:
                algorithm_desc = getattr(function, 'algorithm', None)
                docstring = getattr(function, 'docstring', None)
            except:
                pass
        
        # å‘é€è¯¦ç»†çš„æ›´æ–°åˆ°é˜Ÿåˆ—
        self.output_queue.put({
            "type": "iteration",
            "iteration": self._num_samples,
            "score": score,
            "best_score": self._best_score if self._best_score != float('-inf') else None,
            "is_new_best": is_new_best,
            "code": code_str if is_new_best else None,
            "algorithm": algorithm_desc if is_new_best else None,
            "docstring": docstring if is_new_best else None,
            "elapsed_time": round(elapsed, 1),
            "iter_time": round(iter_time, 2),
        })
        
        # å¦‚æœæœ‰åŸºç¡€ profilerï¼Œä¹Ÿè°ƒç”¨å®ƒ
        if self.base_profiler:
            self.base_profiler.register_function(function, program, **kwargs)
    
    def record_parameters(self, llm, prob, method):
        """è®°å½•å‚æ•°"""
        self.output_queue.put({
            "type": "info",
            "message": f"å‚æ•°å·²è®°å½•: LLM={llm.__class__.__name__}, Method={method.__class__.__name__}"
        })
        if self.base_profiler:
            self.base_profiler.record_parameters(llm, prob, method)
    
    def finish(self):
        """å®Œæˆè¿è¡Œ"""
        total_time = time.time() - self._start_time
        
        # è·å–æœ€ä½³ç®—æ³•çš„æè¿°
        best_algorithm = None
        best_docstring = None
        if self._best_function:
            try:
                best_algorithm = getattr(self._best_function, 'algorithm', None)
                best_docstring = getattr(self._best_function, 'docstring', None)
            except:
                pass
        
        self.output_queue.put({
            "type": "finished",
            "best_score": self._best_score if self._best_score != float('-inf') else None,
            "best_code": str(self._best_function) if self._best_function else None,
            "best_algorithm": best_algorithm,
            "best_docstring": best_docstring,
            "best_program": self._best_program,
            "total_samples": self._num_samples,
            "total_time": round(total_time, 2)
        })
        if self.base_profiler:
            self.base_profiler.finish()
    
    def get_logger(self):
        if self.base_profiler:
            return self.base_profiler.get_logger()
        return None


class AlgorithmRunner:
    """ç®—æ³•è®¾è®¡è¿è¡Œå™¨"""
    
    def __init__(self, 
                 method_name: str,
                 task_name: str,
                 llm_config: Dict[str, Any],
                 parameters: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–ç®—æ³•è¿è¡Œå™¨
        
        Args:
            method_name: æ–¹æ³•åç§°ï¼ˆå¦‚ EoH, FunSearch ç­‰ï¼‰
            task_name: ä»»åŠ¡åç§°ï¼ˆå¦‚ OBPEvaluation ç­‰ï¼‰
            llm_config: LLM é…ç½®ï¼ˆhost, key, modelï¼‰
            parameters: æ–¹æ³•å‚æ•°
        """
        self.method_name = method_name
        self.task_name = task_name
        self.llm_config = llm_config
        self.parameters = parameters or {}
        
        self._output_queue = queue.Queue()
        self._is_running = False
        self._runner_thread = None
        self._result = None
    
    def _import_components(self):
        """åŠ¨æ€å¯¼å…¥ LLM4AD ç»„ä»¶"""
        import inspect
        
        # æ¸…ç†å¯èƒ½å¯¼è‡´åº“å†²çªçš„ç¯å¢ƒå˜é‡
        import os as _os
        env_keys_to_clean = ['DYLD_LIBRARY_PATH', 'LD_LIBRARY_PATH']
        original_env = {}
        for key in env_keys_to_clean:
            if key in _os.environ:
                original_env[key] = _os.environ.pop(key)
        
        try:
            import llm4ad
            
            from llm4ad.task import import_all_evaluation_classes
            from llm4ad.method import import_all_method_classes_from_subfolders
            from llm4ad.tools.llm import import_all_llm_classes_from_subfolders
            from llm4ad.tools.profiler.profile import ProfilerBase
            
            # è·å– llm4ad åŒ…çš„è·¯å¾„
            llm4ad_path = _os.path.dirname(llm4ad.__file__)
            
            # å¯¼å…¥æ‰€æœ‰ç±»
            import_all_evaluation_classes(_os.path.join(llm4ad_path, 'task'))
            import_all_method_classes_from_subfolders(_os.path.join(llm4ad_path, 'method'))
            import_all_llm_classes_from_subfolders(_os.path.join(llm4ad_path, 'tools/llm'))
            
            # è·å–æ‰€æœ‰å¯ç”¨ç±»
            components = {}
            for module in [llm4ad.tools.llm, llm4ad.tools.profiler, llm4ad.task, llm4ad.method]:
                components.update({name: obj for name, obj in vars(module).items() if inspect.isclass(obj)})
            
            return components
        finally:
            # æ¢å¤ç¯å¢ƒå˜é‡
            for key, val in original_env.items():
                _os.environ[key] = val
    
    def _run_internal(self):
        """å†…éƒ¨è¿è¡Œæ–¹æ³•ï¼ˆåœ¨çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰"""
        try:
            # å¯¼å…¥ç»„ä»¶
            components = self._import_components()
            
            # è·å–ç±» - ç›´æ¥ä½¿ç”¨ç±»åï¼ˆç°åœ¨ config_manager å·²ç¡®ä¿ç±»åæ­£ç¡®ï¼‰
            method_class = components.get(self.method_name)
            eval_class = components.get(self.task_name)
            llm_class = components.get('HttpsApi')
            
            if not method_class:
                self._output_queue.put({
                    "type": "error",
                    "message": f"æœªæ‰¾åˆ°æ–¹æ³•ç±»: {self.method_name}ã€‚å¯ç”¨æ–¹æ³•: {[k for k in components.keys() if not k.startswith('_')][:10]}"
                })
                return
            
            if not eval_class:
                self._output_queue.put({
                    "type": "error", 
                    "message": f"æœªæ‰¾åˆ°ä»»åŠ¡ç±»: {self.task_name}"
                })
                return
            
            # åˆ›å»º LLM å®ä¾‹
            llm_instance = llm_class(
                host=self.llm_config.get('host', 'api.bltcy.top'),
                key=self.llm_config.get('key', ''),
                model=self.llm_config.get('model', 'gpt-4o-mini')
            )
            
            # åˆ›å»ºè¯„ä¼°å™¨å®ä¾‹
            eval_instance = eval_class()
            
            # åˆ›å»ºæµå¼ Profiler
            streaming_profiler = StreamingProfiler(self._output_queue)
            
            # å‘é€å¼€å§‹ä¿¡å·
            self._output_queue.put({
                "type": "progress",
                "value": 0,
                "message": "åˆå§‹åŒ–ç®—æ³•è®¾è®¡ç¯å¢ƒ..."
            })
            
            # å‡†å¤‡æ–¹æ³•å‚æ•°
            method_params = {
                'llm': llm_instance,
                'evaluation': eval_instance,
                'profiler': streaming_profiler,
            }
            
            # æ·»åŠ ç”¨æˆ·æŒ‡å®šçš„å‚æ•°
            for key, value in self.parameters.items():
                if key not in ['llm', 'evaluation', 'profiler']:
                    method_params[key] = value
            
            # è®¾ç½®é»˜è®¤å‚æ•°ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
            if 'max_sample_nums' not in method_params:
                method_params['max_sample_nums'] = 50
            if 'num_samplers' not in method_params:
                method_params['num_samplers'] = 2
            if 'num_evaluators' not in method_params:
                method_params['num_evaluators'] = 2
            
            # åˆ›å»ºæ–¹æ³•å®ä¾‹
            method_instance = method_class(**method_params)
            
            # å‘é€å¯åŠ¨ä¿¡å·
            self._output_queue.put({
                "type": "started",
                "method": self.method_name,
                "task": self.task_name,
                "message": f"ğŸš€ å¼€å§‹ä½¿ç”¨ {self.method_name} è®¾è®¡ {self.task_name} çš„ç®—æ³•..."
            })
            
            # è¿è¡Œ
            method_instance.run()
            
            # å®Œæˆ - è°ƒç”¨ profiler.finish()
            streaming_profiler.finish()
            
        except Exception as e:
            import traceback
            self._output_queue.put({
                "type": "error",
                "message": f"è¿è¡Œå‡ºé”™: {str(e)}\n{traceback.format_exc()}"
            })
        finally:
            self._is_running = False
            self._output_queue.put({"type": "done"})
    
    def run_with_stream(self) -> Generator[Dict[str, Any], None, None]:
        """è¿è¡Œç®—æ³•å¹¶æµå¼è¿”å›è¾“å‡º"""
        self._is_running = True
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œ
        self._runner_thread = threading.Thread(target=self._run_internal, daemon=True)
        self._runner_thread.start()
        
        # è¿›åº¦ä¼°è®¡å˜é‡
        last_iteration = 0
        max_samples = self.parameters.get('max_sample_nums', 100)
        
        # ä»é˜Ÿåˆ—ä¸­è¯»å–è¾“å‡º
        while self._is_running or not self._output_queue.empty():
            try:
                update = self._output_queue.get(timeout=0.1)
                
                if update["type"] == "done":
                    break
                
                # ç›´æ¥è½¬å‘æ‰€æœ‰ç±»å‹çš„æ›´æ–°
                yield update
                    
            except queue.Empty:
                continue
        
        # ç­‰å¾…çº¿ç¨‹å®Œæˆ
        if self._runner_thread and self._runner_thread.is_alive():
            self._runner_thread.join(timeout=5)
    
    def run_sync(self) -> Dict[str, Any]:
        """åŒæ­¥è¿è¡Œç®—æ³•"""
        results = list(self.run_with_stream())
        
        # æŸ¥æ‰¾æœ€ç»ˆç»“æœ
        for r in reversed(results):
            if r.get("type") == "result":
                return r.get("data", {})
        
        return {"error": "æœªèƒ½è·å–ç»“æœ"}
    
    def stop(self):
        """åœæ­¢è¿è¡Œ"""
        self._is_running = False


class MockAlgorithmRunner:
    """æ¨¡æ‹Ÿç®—æ³•è¿è¡Œå™¨ï¼Œç”¨äºæµ‹è¯•å’Œæ¼”ç¤º"""
    
    def __init__(self, 
                 method_name: str,
                 task_name: str,
                 llm_config: Dict[str, Any],
                 parameters: Dict[str, Any] = None):
        self.method_name = method_name
        self.task_name = task_name
        self.llm_config = llm_config
        self.parameters = parameters or {}
    
    def run_with_stream(self) -> Generator[Dict[str, Any], None, None]:
        """æ¨¡æ‹Ÿæµå¼è¾“å‡º"""
        max_iterations = self.parameters.get('max_sample_nums', 20)
        
        yield {
            "type": "progress",
            "value": 0,
            "message": "åˆå§‹åŒ–ç®—æ³•è®¾è®¡ç¯å¢ƒ..."
        }
        time.sleep(0.5)
        
        yield {
            "type": "progress",
            "value": 5,
            "message": "å¼€å§‹ç®—æ³•è®¾è®¡..."
        }
        
        best_score = float('-inf')
        mock_codes = [
            '''def priority(item, bins):
    """ä¼˜å…ˆçº§å‡½æ•° v1"""
    return bins - item''',
            '''def priority(item, bins):
    """ä¼˜å…ˆçº§å‡½æ•° v2"""
    return (bins - item) / (bins + 1)''',
            '''def priority(item, bins):
    """ä¼˜å…ˆçº§å‡½æ•° v3"""
    mask = bins >= item
    scores = np.where(mask, bins - item, -np.inf)
    return scores''',
            '''def priority(item, bins):
    """ä¼˜å…ˆçº§å‡½æ•° v4 - æ”¹è¿›ç‰ˆ"""
    remaining = bins - item
    mask = remaining >= 0
    # åå¥½å‰©ä½™ç©ºé—´å°çš„ç®±å­
    scores = np.where(mask, -remaining / (bins + 1), -np.inf)
    return scores''',
        ]
        
        for i in range(min(max_iterations, 10)):
            time.sleep(0.3)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
            
            # æ¨¡æ‹Ÿå¾—åˆ†æå‡
            import random
            score = -120 + i * 5 + random.random() * 10
            if score > best_score:
                best_score = score
            
            progress = 10 + int(85 * (i + 1) / max_iterations)
            
            yield {
                "type": "progress",
                "value": progress,
                "message": f"ç¬¬ {i+1} æ¬¡è¿­ä»£ï¼Œå½“å‰æœ€ä½³å¾—åˆ†: {best_score:.2f}"
            }
            
            yield {
                "type": "log",
                "iteration": i + 1,
                "score": score,
                "best_score": best_score,
                "code": mock_codes[i % len(mock_codes)],
                "algorithm": f"å¯å‘å¼ç­–ç•¥ v{i+1}"
            }
        
        yield {
            "type": "progress",
            "value": 100,
            "message": "ç®—æ³•è®¾è®¡å®Œæˆï¼"
        }
        
        # æœ€ç»ˆç»“æœ
        yield {
            "type": "result",
            "data": {
                "best_score": best_score,
                "best_code": mock_codes[-1],
                "total_samples": max_iterations,
                "total_time": max_iterations * 0.3
            }
        }


def create_runner(method_name: str,
                  task_name: str, 
                  llm_config: Dict[str, Any],
                  parameters: Dict[str, Any] = None,
                  use_mock: bool = False) -> AlgorithmRunner:
    """åˆ›å»ºç®—æ³•è¿è¡Œå™¨çš„å·¥å‚å‡½æ•°"""
    if use_mock:
        return MockAlgorithmRunner(method_name, task_name, llm_config, parameters)
    return AlgorithmRunner(method_name, task_name, llm_config, parameters)
